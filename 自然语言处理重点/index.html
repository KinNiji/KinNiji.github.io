<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Niji"><title>自然语言处理重点 · NijiBlog</title><meta name="description" content="大纲
NLP基础与机器学习

NLP的基本任务（给予描述、说出是哪种任务）
机器学习评价指标：ACC、F1（怎么算的）
中文分词（有哪些方法）

词向量表示

离散表示

OneHot
Bag of Words： TF-IDF（怎么计算）
N-gram：数据平滑
优缺点"><meta name="keywords" content="Hexo,Blog"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">NijiBlog</a></h3><div class="description"><p>Nothing lasts forever.</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="http://weibo.com/7276000901"><i class="fa fa-weibo"></i></a></li><li><a target="_blank" rel="noopener" href="http://github.com/KinNiji"><i class="fa fa-github"></i></a></li></ul><div class="footer"><span>Powered by</span><a href="https://hexo.io/zh-cn/" target="_blank"> Hexo</a><span> &</span><a target="_blank" rel="noopener" href="https://github.com/Ben02/hexo-theme-Anatole"> Anatole</a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/profile.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>自然语言处理重点</a></h3></div><div class="post-content"><h1 id="大纲">大纲</h1>
<p>NLP基础与机器学习</p>
<ul>
<li>NLP的基本任务（给予描述、说出是哪种任务）</li>
<li>机器学习评价指标：ACC、F1（怎么算的）</li>
<li>中文分词（有哪些方法）</li>
</ul>
<p>词向量表示</p>
<ul>
<li>离散表示
<ul>
<li>OneHot</li>
<li>Bag of Words： TF-IDF（怎么计算）</li>
<li>N-gram：数据平滑</li>
<li>优缺点</li>
</ul></li>
<li>分布式表示
<ul>
<li>分布式假设</li>
<li>word2vec：
<ul>
<li>skip-gram、CBOW</li>
<li>负采样、层次SoftMax</li>
</ul></li>
<li>FastText</li>
<li>Glove</li>
</ul></li>
</ul>
<p>CNN</p>
<ul>
<li>卷积怎么扩大感受野</li>
<li>参数计算（给予输入算输出、算参数量）</li>
<li>TextCNN（结构，用PyTorch如何写）</li>
</ul>
<p>RNN</p>
<ul>
<li>梯度消失、梯度爆炸（如何解决）</li>
<li>LSTM结构（重点、为什么能够避免上述问题）</li>
<li>GRU结构（了解）</li>
</ul>
<p>Seq2Seq</p>
<ul>
<li>Seq2Seq结构（encoder、decoder）</li>
<li>解码方式</li>
<li>训练和推理（如何训练和测试、训练和测试为什么不一致）</li>
<li>Seq2Seq+Attention结构（在哪里加Attention）</li>
</ul>
<p>transformer</p>
<ul>
<li>transformer结构（部分去除后补充完整）</li>
<li>Self Attention、multi-head Attention、Mask Self Attention（区别）</li>
<li>Layer Normalization</li>
<li>残差连接</li>
</ul>
<p>BERT</p>
<ul>
<li>迁移学习、多任务学习</li>
<li>ELMO、BERT、GPT结构</li>
<li>BERT、ELMO的训练任务、使用方法</li>
</ul>
<h1 id="选择2x10">选择2’x10</h1>
<p>假设有一个5层的神经网络，4GB显存3小时完成训练，单个数据2s，现变幻架构，当评分是0.2和0.3时，分别在第二层和第四层添加dropout，新架构所用时间是多少？</p>
<p>会使用哪项技术来调整超参来最小化损失函数？</p>
<p>指出正确的训练测试代码</p>
<h1 id="判断2x5">判断2’x5</h1>
<p>某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性越高。【对】</p>
<p>混沌度是一种常见的应用在处理NLP问题过程中的评估技术，混沌度越低越好。【对】</p>
<p>循环神经网络模型参数的数量随文本长度的增加而增长。【错，参数量不变】</p>
<h1 id="填空2x5">填空2’x5</h1>
<p>词向量的评价方法：</p>
<blockquote>
<p>【类比推理】、【相似度评价】</p>
</blockquote>
<p>seq2seq两种训练方式：</p>
<blockquote>
<p>【Teacher forcing】和【Free running】</p>
</blockquote>
<p>LSTM中遗忘门的激活函数：</p>
<blockquote>
<p>【sigmoid】</p>
</blockquote>
<p>机器翻译的评价标准：</p>
<blockquote>
<p>【困惑度】、【BELU】</p>
</blockquote>
<p>BERT的三个输入：</p>
<blockquote>
<p>【词嵌入（包括token）】、【片段嵌入】、【位置嵌入】</p>
</blockquote>
<p>迁移学习的分类：</p>
<blockquote>
<p>【样本迁移】、【特征迁移】、【模型迁移】、【关系迁移】</p>
</blockquote>
<p>多任务学习四种共享模式：</p>
<blockquote>
<p>【硬共享模式】、【软共享模式】、【层次共享模式】、【共享-私有模式】</p>
</blockquote>
<p>多任务学习的两个阶段：</p>
<blockquote>
<p>【联合训练阶段】、【单任务微调阶段】</p>
</blockquote>
<h1 id="简答10x6">简答10’x6</h1>
<p>Word2vec包含哪两种模型？并简述其基本思想</p>
<blockquote>
<p>word2vec包含跳字模型(skip-gram)和连续词袋(CBOW)模型。</p>
<p>连续词袋模型假设基于背景词来生成中心词。</p>
<p>跳字模型假设基于中心词来生成背景词。</p>
</blockquote>
<p>当词典很大时如何降低模型的计算量?</p>
<blockquote>
<p>负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关。</p>
<p>层次SoftMax使用了二叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关。</p>
</blockquote>
<p>哪一种模型更能有效表示低频词?</p>
<blockquote>
<p>Hierarchical SoftMax（层次SoftMax）通过Huffman树结构来计算所有词的概率来定义目标，对低频词更友好。</p>
</blockquote>
<p>FastText在word2vec模型基础上进行了什么改进？</p>
<blockquote>
<p>FastText提出了子词嵌入法。它在word2vec中的跳字模型的基础上，将中心词向量表示成单词的子词向量之和。</p>
</blockquote>
<p>给出两种评价词嵌入模型性能的方法</p>
<blockquote>
<p>类比推理：考察用词向量来推断不同单词之间的语义关系的能力，在这个任务中，三个单词a, b和s被给出，目标是推断出第四个单词t满足“a和b, t和s是相似的”。</p>
<p>相似度评价：评估词向量模型在两个词之间的语义紧密度和相关性的能力，例如男人与女人，中国与北京这些词对之间的相似度。在词相似度任务上，一般采用斯皮尔曼等级相关系数ρ (Spearman's rank correlation coefficient)作为评价指标，它是衡量两个变量的依赖性的指标，利用单调方程评价两个统计变量的相关性，如果数据中没有重复值，并且当两个变量完全单调相关时，斯皮尔曼相关系数则为+1或-1。</p>
</blockquote>
<p>相比于全连接，CNN 为什么可以减少参数?</p>
<blockquote>
<p>局部连接</p>
<p>权重共享</p>
<p>空间或时间上的次采样</p>
</blockquote>
<p>神经网络中Dropout的作用?具体是怎么实现的?</p>
<blockquote>
<p>当训练一个深度神经网络时，我们可以随机丟弃一部分神经元(同时丢弃其对应的连接边)来避免过拟合，这种方法称为丢弃法Dropout Method)。每做一次丢弃，相当于从原始的网络中采样得到一个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数。那么，最终的网络可以近似看作集成了多个不同网络的组合模型。</p>
</blockquote>
<p>解释批量归一化的原理</p>
<blockquote>
<p>更好的尺度不变性：在深度神经网络中，一个神经层的输入是之前神经层的输出，当使用随机梯度下降来训练网络时，每次参数更新都会导致该神经层的输入分布发生改变。越高的层，其输入分布会改变得越明显。对每一个神经层的输入进行归一化操作，使其分布保持稳定。</p>
<p>更平滑的优化地形：逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面还可以使得神经网络的优化地形(Optimization Landscape)更加平滑，以及使梯度变得更加稳定，从而允许我们使用更大的学习率，并提高收敛速度。</p>
</blockquote>
<p>1x1卷积作用</p>
<blockquote>
<p>调节通道数：由于1x1卷积并不会改变高和宽，改变通道的第一个最直观的结果，就是可以将原本的数据量进行增加或者减少。</p>
<p>增加非线性：1x1卷积核，可以在保持特征图尺度不变的(即不改变)的前提下大幅增加非线性特性（利用后接的非线性激活函数如ReLU）。 非线性允许网络学习更复杂的功能，并且使得整个网络能够进一步加深。</p>
<p>跨通道信息：使用1x1卷积核，实现降维和升维的操作其实就是间通道信息的线性组合变化。</p>
<p>减少参数：前面所说的降维，其实也是减少了参数，因为特征图少了，参数也自然跟着就减少，相当于在特征图的通道数上进行卷积，压缩特征图，二次提取特征，使得新特征图的特征表达更佳。</p>
</blockquote>
<p>卷积神经网络如何用于文本分类任务?</p>
<blockquote>
<p>卷积神经网络的核心思想是捕捉局部特征，起初在图像领域取得了巨大的成功，后来在文本领域也得到了广泛的应用。对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文 本分类任务中取得了非常不错的效果。</p>
</blockquote>
<p>给出一个防止过拟合的方法，是什么原理</p>
<blockquote>
<p>Dropout：见上</p>
</blockquote>
<p>RNN长程依赖问题</p>
<blockquote>
<p>RNN由于梯度爆炸或消失问题，实际上只能学习到短周期的依赖关系。</p>
</blockquote>
<p>如何解决梯度消失和梯度爆炸</p>
<blockquote>
<p>权重正则化</p>
<p>relu等激活函数</p>
<p>batch normalization</p>
<p>残差连接</p>
</blockquote>
<p>LSTM是如何实现长短期记忆的? (遗忘门和输入门的作用)</p>
<blockquote>
<p>LSTM主要通过遗忘门和输入门来实现长短期记忆。如果当前时间点的状态中没有重要信息，遗忘门f中各分量的值将接近1(f-&gt;1) ;输入门i中各分量的值将接近0(i-&gt;0）;此时过去的记忆将会被保存，从而实现长期记忆。</p>
<p>如果当前时间点的状态中出现了重要信息，且之前的记忆不再重要,则f-&gt;0， i-&gt;1;此时过去的记忆被遗忘，新的重要信息被保存，从而实现短期记忆。</p>
<p>如果当前时间点的状态中出现了重要信息，但旧的记忆也很重要，则f-&gt;1，i-&gt;1。</p>
</blockquote>
<p>LSTM里各部分使用了不同的激活函数，为什么?</p>
<blockquote>
<p>Sigmoid层决定什么值需要更新，Tanh层创建一个新的候选值向量。</p>
<p>Sigmoid的输出在0~1，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关，在生成候选记亿时。</p>
<p>tanh的输出在-1~-1，这与大多数场景下特征分布是0中心的吻合。此外，tanh函数在输入为0近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。</p>
</blockquote>
<p>NER给定包含n个词的一句话，使用RNN对每一个词做人名，组织机构，其他命名实体识别任务，请简述其输出</p>
<blockquote>
<p>句子中的每个单词都被输入RNN，井且在每个时间步产生一个输出，对应于每个单词的预测标签的发射矩阵。</p>
</blockquote>
<p>NER中为什么要用CRF</p>
<blockquote>
<p>CRF层可以加入一些约束来保证最终预测结果是有效的，大大减少错误的预测序列。</p>
</blockquote>
<p>Seq2seq中为什么训练和预测时的Decoder不一样</p>
<blockquote>
<p>训练时使用Teacher forcing会使训练过程更快地收敛。</p>
</blockquote>
<p>Attention中为什么要除以<span class="math inline">\(\sqrt{d_k}\)</span></p>
<blockquote>
<p><span class="math inline">\(Attention(Q, K, V)=SoftMax(\frac{QK^T}{\sqrt{d_k}})V\)</span>，<span class="math inline">\(d_k\)</span>表示K的维度</p>
<p><span class="math inline">\(d_k\)</span>较大时，向量内积容易取很大的值（query和key向量中的元素都是相互独立的均值为0，方差为1的随机变量，那么这两个向量的内积的均值为0，而方差为<span class="math inline">\(d_k\)</span>），SoftMax函数梯度很小趋于0。</p>
<p>除以<span class="math inline">\(\sqrt{d_k}\)</span>使得方差稳定到1，SoftMax的梯度不至于太小，有效控制了梯度消失问题。</p>
</blockquote>
<p>Multi-head Attention的作用</p>
<blockquote>
<p>在某种意义上防止了过拟合。</p>
<p>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。</p>
</blockquote>
<p>Masked Attention的作用</p>
<blockquote>
<p>使得decoder不能看见未来的信息。在t时刻，解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。</p>
</blockquote>
<p>Yesterday turned out to be a terrible day. I overslept my alarm clock, and to make matters worse, my dog ate my homework. At least my dog seems happy.给定以上语句，使用RNN做情感分析得到是类别为：积极。请问什么原因?</p>
<blockquote>
<p>这句话的最后一个词是“happy"，属于非常积极的情绪词。由于我们只使用最终隐藏状态来计算输出，因此最终单词在分类中会产生太大的影响。此外，由于句子很长，早期时间步的信息可能由于消失梯度问题而无法保存。</p>
</blockquote>
<p>若更换Self-Attention中输入序列的顺序，其QKV和输出会发生什么变化</p>
<blockquote>
<p>Query、Key向量、Value向量和输出向量只是顺序发生变化</p>
</blockquote>
<p>ELMO结构及输入</p>
<blockquote>
<p>ELMO由一层input层和两层LSTM组合而成的。</p>
</blockquote>
<p>ELMO如何进行预训练，如何使用输出?</p>
<blockquote>
<p>ELMO的三层每一层都有一个输出，将这三层的输出按比例相加后即为所得vector。 这个比例是模型学习得到的。得到加权后的向量后，如何使用取决于任务的效果。</p>
</blockquote>
<p>为什么ELMO用两个单向的LSTM代替一个双向的LSTM</p>
<blockquote>
<p>用双向的模型结构去训练语言模型会导致“看到自己”或“看到答案”的问题。对于多层LSTM来说，涉及到序列前后信息泄露的问题，深层双向LSTM会被泄露上下文词语的位置信息，导致模型学到了不该学习的东西，也就失去了预训练的效果。</p>
</blockquote>
<p>多任务学习通常可以获得比单任务学习更好的泛化能力，主要是什么原因</p>
<blockquote>
<p>多任务学习在多个任务的数据集上进行训练，比单任务学习的训练集更大，由于多个任务之间有一定的相关性，因此多 任务学习相当于是一种隐式的数据增强，可以提高模型的泛化能力。</p>
<p>多任务学习中的共享模块需要兼顾所有任务，这在一定程度上避免了模型过拟合到单个任务的训练集，可以看作一种正则化。</p>
<p>既然一个好的表示通常需要适用于多个不同任务，多任务学习的机制使得它会比单任务学习获得更好的表示。</p>
<p>在多任务学习中， 每个任务都可以“选择性”利用其他任务中学习到的隐藏特征，从而提高自身的能力。</p>
</blockquote>
<p>设计一个文本匹配模型，给出输入、输出、损失函数</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-07-01</span><i class="fa fa-tag"></i><a class="tag" href="/categories/笔记/" title="笔记">笔记 </a><a class="tag" href="/tags/NLP/" title="NLP">NLP </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://KinNiji.github.io/自然语言处理重点/,NijiBlog,自然语言处理重点,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" title="自然语言处理">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" title="操作系统">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>