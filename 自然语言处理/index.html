<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Niji"><title>自然语言处理 · NijiBlog</title><meta name="description" content="NLP基础与机器学习

评价指标

中文分词方法：
基于词典的分词
基于统计的机器学习算法
自然语言模型与词向量
离散表示
OneHot模型
保留了原始句子的所有细节，包括语法和词序。
缺点是词表增大时，表示矩阵也会增大。
词袋模型
计算词在给定文本中出现的次数或者频率。"><meta name="keywords" content="Hexo,Blog"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">NijiBlog</a></h3><div class="description"><p>Nothing lasts forever.</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="http://weibo.com/7276000901"><i class="fa fa-weibo"></i></a></li><li><a target="_blank" rel="noopener" href="http://github.com/KinNiji"><i class="fa fa-github"></i></a></li></ul><div class="footer"><span>Powered by</span><a href="https://hexo.io/zh-cn/" target="_blank"> Hexo</a><span> &</span><a target="_blank" rel="noopener" href="https://github.com/Ben02/hexo-theme-Anatole"> Anatole</a></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/profile.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>自然语言处理</a></h3></div><div class="post-content"><h1 id="nlp基础与机器学习">NLP基础与机器学习</h1>
<figure>
<img src="../images/评价指标.png" alt="评价指标" /><figcaption aria-hidden="true">评价指标</figcaption>
</figure>
<p>中文分词方法：</p>
<p>基于词典的分词</p>
<p>基于统计的机器学习算法</p>
<h1 id="自然语言模型与词向量">自然语言模型与词向量</h1>
<h2 id="离散表示">离散表示</h2>
<h3 id="onehot模型">OneHot模型</h3>
<p>保留了原始句子的所有细节，包括语法和词序。</p>
<p>缺点是词表增大时，表示矩阵也会增大。</p>
<h3 id="词袋模型">词袋模型</h3>
<p>计算词在给定文本中出现的次数或者频率。</p>
<p>假设：一个词在文档中出现的次数越多，那么该词对文档的意义的贡献就越大。</p>
<h4 id="齐普夫定律">齐普夫定律</h4>
<p>齐普夫定律：在一个大的语料库中统计词频，然后将词按照词频从高到低的顺序排列成一张表。一个词的词频 f 和它在表中的序号 r 之间存在关系：<span class="math inline">\(f∝ \frac{1}{r}\)</span> 或 <span class="math inline">\(f \times r≈k\)</span>， k为常数。</p>
<p>在文本文档中，各频次词语以一定规律分布，词频统计方法利用统计学知识对词汇规律进行描述。齐普夫定律是对人类语言词频分布的一个粗糙而有用的描述:</p>
<ul>
<li>非常常用的词很少</li>
<li>中频词的数量中等</li>
<li>大量低频词</li>
</ul>
<p>省力原则（the Principle of Least Effort）：以最小的代价换取最大的收益。这是指导人类行为的一条根本性原则，各种资源(人力、货物、时间、技巧以及其他任何生产性资源)都存在一种进行自我调整以实现最小化工作量的趋向。</p>
<ul>
<li><p>讲者希望：用一个词表达所有的意义是最经济的。这样，说话人不需要花费气力去掌握更多的词汇，也不需要考虑如何从一堆词汇中选择一个合适的词。</p></li>
<li><p>听者希望：最省力的是每个词都只有一一个意义，词汇的形式和意义之间完全一一对应。</p></li>
</ul>
<h4 id="词项频率-tf">词项频率 TF</h4>
<p>某个词在给定文档中出现等次数为词项频率</p>
<p>归一化词频：<span class="math inline">\(TF = \frac{词t在文档中出现的次数}{文档总词数}\)</span></p>
<h4 id="逆文档频率-idf">逆文档频率 IDF</h4>
<p>逆文档频率：反映一个关键词在整个数据全局中重要性的全局性统计特征。</p>
<p>词计数是有用的，但是纯词计数，即使按照文档长度进行归一化处理，也不能告诉我们太多该词在当前文档相对于语料库中其他文档的重要信息。如果一个词在整个数据全集中出现的频度很小，则它应该是反映包含该类词的文档内容的重要词汇。因此，一个关键词的权重应该与该词所在的文档的总数成反比或近似反比的关系：<span class="math inline">\(IDF = log(\frac{N}{n_i}) = log(\frac{文档数}{包含词t的文档数})\)</span></p>
<p>如果一个词在某一个文档中出现很多次，但是在其他文档中几乎很少出现，那么可以认为这个词对当前文档很重要。</p>
<h4 id="tf-idf">TF-IDF</h4>
<p>对于语料库 D 中给定的文档 d 中的词 t，有</p>
<p><span class="math inline">\(TF(t,d) = log(\frac{t在d中出现的次数}{d的总词数})\)</span></p>
<p><span class="math inline">\(IDF(t,D) = log(\frac{D的总文档数}{包含t的文档数})\)</span></p>
<p><span class="math inline">\(TF-IDF(t,d,D) = TF(t,d) \times IDF(t,D)\)</span></p>
<h3 id="n-gram模型">N-gram模型</h3>
<h4 id="句子概率">句子概率</h4>
<figure>
<img src="../images/句子概率.png" alt="句子概率" /><figcaption aria-hidden="true">句子概率</figcaption>
</figure>
<p>给定一个句子中前面 n-1个词，预测下面的词是哪个词。由于语言的规律性，句子中前面出现的词对后面可能出现的词有很强的预示作用。</p>
<figure>
<img src="../images/N-gram模型.png" alt="N-gram模型" /><figcaption aria-hidden="true">N-gram模型</figcaption>
</figure>
<ul>
<li>n较大时
<ul>
<li>提供了更多的语境信息，语境更具区别性。</li>
<li>参数个数多、计算代价大、训练语料需要多、参数估计不可靠。</li>
</ul></li>
<li>n较小时
<ul>
<li>语境信息少，不具区别性。</li>
<li>参数个数少、计算代价小、训练语料无需太多、参数估计可靠。</li>
</ul></li>
</ul>
<h4 id="建立n-gram">建立N-gram</h4>
<p>数据准备:</p>
<ul>
<li>确定训练语料</li>
<li>对语料进行tokenization或切分</li>
<li>句子边界，增加两个特殊的词[B]和[E]</li>
</ul>
<figure>
<img src="../images/建立N-gram1.png" alt="建立N-gram" /><figcaption aria-hidden="true">建立N-gram</figcaption>
</figure>
<figure>
<img src="../images/建立N-gram2.png" alt="建立N-gram" /><figcaption aria-hidden="true">建立N-gram</figcaption>
</figure>
<figure>
<img src="../images/建立N-gram3.png" alt="建立N-gram" /><figcaption aria-hidden="true">建立N-gram</figcaption>
</figure>
<p>最大似然估计(MLE)给训练样本中未观察到的事件赋以0概率。若某N-gram在训练语料中没有出现，则该N-gram的概率必定是0。解决的办法是扩大训练语料的规模。但是无论怎样扩大训练语料，都不可能保证所有的词在训练语料中均出现。</p>
<p>由于训练样本不足而导致所估计的分布不可靠的问题，称为<strong>数据稀疏问题</strong>。在NLP领域中，数据稀疏问题永远存在，不太可能有一个足够大的训练语料，因为语言中的大部分词都属于低频词(齐普夫定律)。</p>
<h4 id="平滑技术">平滑技术</h4>
<p>为了解决数据稀疏问题，把在训练样本中出现过的事件的概率适当减小。把减小得到的概率密度分配给训练语料中没有出现过的事件。这个过程有时也称为discounting(减值)。</p>
<p>Add-one 平滑：每一种情况出现的次数加1。</p>
<p>训练语料中未出现的n- gram的概率不再为0，而是一一个大于0的较小的概率值。但由于训练语料中未出现n-gram数量太多，平滑后，所有未出现的n-gram占据了整个概率分布中的一个很大的比例，给训练语料中没有出现过的n-gram分配了太多的概率空间。</p>
<p>认为所有未出现的n-gram概率相等不合理；出现在训练语料中的n-gram，都增加同样的频度值不公平</p>
<p>Add-delta 平滑：加一个小于1的正数δ，效果比Add-one好，但是仍然不理想。</p>
<h3 id="统计语言模型的优缺点">统计语言模型的优缺点</h3>
<p>优点：</p>
<ul>
<li>统计语言模型非常容易训练(通过计数来估计概论)</li>
<li>在实际应用中有较好的效果</li>
</ul>
<p>缺点:</p>
<ul>
<li>模型复杂度较高: n-gram O(|V|^n)</li>
<li>模型的泛化性能差</li>
<li>平滑技术的适用性差</li>
</ul>
<h2 id="分布式表示">分布式表示</h2>
<h3 id="分布式假设">分布式假设</h3>
<p>一个词的含义可以通过它在语料中所有出现的上下文的聚合来表示，因此出现在相似上下文中的词的意义也相似。</p>
<h3 id="静态词向量">静态词向量</h3>
<p>词向量：是对词语语义或含义的数值向量表示，包括字面意义和隐含意义。词向量也称之为词嵌入(word embeddings)或者词表示(word representations)。</p>
<ul>
<li>把词表示成低维空间中的向量(100-1000)</li>
<li>句法语义特性相近的词在空间中距离相近</li>
<li>更为细致的语义结构信息——类比特性</li>
<li>分布假设(Distributional Hypothesis)：一个词的含义可以通过它在语料中所有出现的上下文的聚合来表示，因此出现在相似上下文中的词的意义也相似</li>
</ul>
<p>每一个词被表示为一个低维向量空间中的稠密向量，目的是使意义相近的词在空间中位置也相近。</p>
<h4 id="word2vec">Word2vec</h4>
<p>大规模未标注语料库中，每一句语料中的任意位置都有一个中心词(center word)c和上下文词(context word)o，c和o的相似度可用给定c的情况下o的条件概率来表示，不断的调整word vector使得这个概率最大化。每个单词w可用两个向量表示，一个是它作为center word时的向量Vw以及它作为context word时的向量Uw。</p>
<p><strong>Word2vec实现：</strong></p>
<ul>
<li><strong>CBOW（continuous bag-of-words）是根据中心词周围的上下文单词来预测该词的词向量。训练速度快。</strong></li>
<li><strong>Skip-Gram是根据中心词预测周围上下文的词的概率分布。处理生僻字能力高。</strong></li>
</ul>
<p><strong>训练方法：</strong></p>
<ul>
<li><strong>Negative Sampling（负采样）：通过抽取负样本来定义目标。对高频词更友好。</strong></li>
<li><strong>Hierarchical SoftMax（层次SoftMax）：通过Huffman树结构来计算所有词的概率来定义目标。对低频词更友好。</strong></li>
</ul>
<p>局限性：</p>
<ul>
<li>只考虑到上下文信息，忽略了全局信息。</li>
<li>只考虑上下文的共现性，忽略了顺序信息。</li>
</ul>
<h4 id="fasttext">FastText</h4>
<p>在word2vec中并没有直接利用构词学中的信息。无论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog” 和“dogs"分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。FastText提出了<strong>子词嵌入</strong>(sub word embedding)的方法， 从而试图将构词信息引入word2vec中的跳字模型。在FastText中，每个中心词被表示成子词的集合。FastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。</p>
<p>优点：</p>
<ul>
<li><p>学习速度比较快</p></li>
<li><p>适用于分类类别非常大而且数据集足够多的情况</p></li>
</ul>
<p>缺点：</p>
<ul>
<li>当分类类别比较小或者数据集比较少的话，很容易过拟合</li>
</ul>
<h4 id="glove">GloVe</h4>
<p>Global Vectors，全局向量的词嵌入，GloVe是一种基于全局语料计数的无监督学习模型，但是采用log(X)作为label，故损失函数采用最小平方损失函数。从全局考虑输出词向量表征。</p>
<p>Word2vec与GloVe：</p>
<ul>
<li>word2Vec 是一种预测型模型，而GloVe是基于计数的模型。</li>
<li>word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建共现矩阵，是基于全局语料的，可见GloVe需要事先统计共现概率。因此，word2vec可以进行在线学习，GloVe则需要统计固定语料信息。</li>
<li>word2vec是无监督学习，同样由于不需要人工标注；GloVe通常被认为是无监督学习，但实际上GloVe还是有label的，即共现次数。</li>
<li>word2vec损失函数实质上是带权重的交叉熵，权重固定；GloVe的损失函数是最小平方损失函数，权重可以做映射变换。</li>
<li>总体来看，GloVe可以被看作是更换了目标函数和权重函数的全局word2vec。</li>
</ul>
<p>优点：</p>
<ul>
<li>快速训练</li>
<li>可扩展到大型语料库</li>
<li>即使是小的语料库和小的向量也表现良好</li>
<li>早停。当改进变小时，我们可以停止培训。</li>
</ul>
<p>缺点：</p>
<ul>
<li>占用大量内存：构造术语共生矩阵的最快方法是将其作为哈希图保存在RAM中，并以全局方式执行共生增量</li>
<li>有时对初始学习率非常敏感</li>
</ul>
<h3 id="词向量评价">词向量评价</h3>
<h4 id="类比推理">类比推理</h4>
<p>考察用词向量来推断不同单词之间的语义关系的能力，在这个任务中，三个单词a, b和s被给出，目标是推断出第四个单词t满足“a和b, t和s是相似的”。</p>
<h4 id="相似度评价">相似度评价</h4>
<p>评估词向量模型在两个词之间的语义紧密度和相关性的能力，例如男人与女人，中国与北京这些词对之间的相似度。在词相似度任务上，一般采用斯皮尔曼等级相关系数ρ (Spearman's rank correlation coefficient)作为评价指标，它是衡量两个变量的依赖性的指标，利用单调方程评价两个统计变量的相关性，如果数据中没有重复值，并且当两个变量完全单调相关时，斯皮尔曼相关系数则为+1或-1。</p>
<h1 id="卷积神经网络">卷积神经网络</h1>
<p>自然图像中的物体具有<strong>局部不变特征</strong>。</p>
<h2 id="卷积">卷积</h2>
<p><strong>卷积神经网络结构特性：</strong></p>
<ul>
<li><strong>局部连接</strong></li>
<li><strong>权重共享</strong></li>
<li><strong>空间或时间上的次采样</strong></li>
</ul>
<p><strong>扩大感受野：</strong></p>
<ul>
<li><strong>增加卷积核大小</strong></li>
<li><strong>增加层数</strong></li>
<li><strong>卷积前池化</strong></li>
<li><strong>空洞卷积</strong></li>
</ul>
<p>参数：</p>
<ul>
<li>in_size M0</li>
<li>out_size M1</li>
<li>in_channels D</li>
<li>out_channels 卷积核数量 K</li>
<li>kernel_size F</li>
<li>stride S</li>
<li>padding P</li>
</ul>
<p>计算：</p>
<ul>
<li><p>==<span class="math inline">\(M_1=\lfloor \frac{M_0+2P-F}{S}\rfloor+1\)</span>==</p></li>
<li><p>==<span class="math inline">\(参数量=(F^2D + 1)K\)</span>​，+1为偏置项，每个卷积核一个==</p></li>
</ul>
<p>1x1卷积：</p>
<ul>
<li>调节通道数：由于1x1卷积并不会改变高和宽，改变通道的第一个最直观的结果，就是可以将原本的数据量进行增加或者减少。</li>
<li>增加非线性：1x1卷积核，可以在保持特征图尺度不变的(即不改变)的前提下大幅增加非线性特性（利用后接的非线性激活函数如ReLU）。 非线性允许网络学习更复杂的功能，并且使得整个网络能够进一步加深。</li>
<li>跨通道信息交互：使用1x1卷积核，实现降维和升维的操作其实就是channel间通道信息的线性组合变化。</li>
<li>减少参数：前面所说的降维，其实也是减少了参数，因为特征图少了，参数也自然跟着就减少,相当于在特征图的通道数上进行卷积，压缩特征图，二次提取特征，使得新特征图的特征表达更佳。</li>
</ul>
<p>卷积方式：</p>
<ul>
<li>窄卷积：S=1 P=0 输出M-F+1</li>
<li>宽卷积：S=1 P=F-1 输出M+F-1</li>
<li>等宽卷积：S=1 P=(F-1)/2 输出M</li>
<li>转置卷积：低维特征映射到高维特征</li>
<li>空洞卷积：dilation</li>
</ul>
<h2 id="池化">池化</h2>
<p>向下采样Subsampling不会改变需要识别的目标，让图像变小了，需要更少的数据描述一个图像。</p>
<p>计算：</p>
<ul>
<li><span class="math inline">\(输出尺寸=向下取整(\frac{M-F}{S})+1\)</span></li>
<li><span class="math inline">\(输出维度=D\)</span></li>
</ul>
<h2 id="卷积网络结构">卷积网络结构</h2>
<p>卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。</p>
<p>典型结构</p>
<figure>
<img src="../images/卷积网络结构.png" alt="卷积网络结构" /><figcaption aria-hidden="true">卷积网络结构</figcaption>
</figure>
<h2 id="经典cnn">经典CNN</h2>
<ul>
<li>LeNet5</li>
<li>AlexNet</li>
<li>VGG16</li>
<li>GoogleNet（Inception网络）：在卷积网络中，如何设置卷积层的卷积核大小是一个十分关键的问题。在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块。Inception 模块同时使用1x1、3x3、5x5等不同大小的卷积核，并将得到的特征映射在深度上拼接(堆叠)起来作为输出特征映射。用多层的小卷积核来替换大的卷积核，以减少计算量和参数量。使用两层3x3的卷积来替换5x5的卷积；使用连续的nx1和1xn来替换nxn的卷积。</li>
<li>ResNet：残差网络(Residual Network，ResNet)是通过给非线性的卷积层增加直连边的方式来提高信息的传播效率，假设在一个深度网络中，我们期望一个非线性单元(可以为一层或多层的卷积层)f(x,θ)去逼近一个目标函数为h(x)。将目标函数拆分成两部分：恒等函数和残差函数。</li>
</ul>
<h2 id="文本cnn">文本CNN</h2>
<h3 id="textcnn">TextCNN</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        n_vocab=<span class="literal">None</span>,  <span class="comment"># 词表长度</span></span></span><br><span class="line"><span class="params">        embed_dim=<span class="number">300</span>,  <span class="comment"># 词嵌入维度</span></span></span><br><span class="line"><span class="params">        num_filters=<span class="number">256</span>,  <span class="comment"># 卷积核数量</span></span></span><br><span class="line"><span class="params">        filter_sizes=(<span class="params"><span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span>),  <span class="comment"># 卷积核尺寸</span></span></span><br><span class="line"><span class="params">        dropout_rate=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="literal">None</span>  <span class="comment"># 类别数</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(TextCNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(</span><br><span class="line">            num_embeddings=n_vocab,  <span class="comment"># 词表长度</span></span><br><span class="line">            embedding_dim=embed_dim,</span><br><span class="line">            padding_idx=n_vocab-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.convs = nn.ModuleList([nn.Conv2d(<span class="number">1</span>, num_filters, (k, embed_dim)) <span class="keyword">for</span> k <span class="keyword">in</span> filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(dropout_rate)</span><br><span class="line">        self.fc = nn.Linear(num_filters * <span class="built_in">len</span>(filter_sizes), num_classes)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">conv_and_pool</span>(<span class="params">x, conv</span>):</span><br><span class="line">        x = F.relu(conv(x)).squeeze(<span class="number">3</span>)</span><br><span class="line">        x = F.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.embedding(x[<span class="number">0</span>])</span><br><span class="line">        out = out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="siamese">Siamese</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SiaGRU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        n_vocab=<span class="literal">None</span>,  <span class="comment"># 词表长度</span></span></span><br><span class="line"><span class="params">        embed_dim=<span class="number">300</span>,  <span class="comment"># 词嵌入维度</span></span></span><br><span class="line"><span class="params">        embed_dropout_rate=<span class="number">0.2</span>,</span></span><br><span class="line"><span class="params">        hidden_size=<span class="number">300</span>,  <span class="comment"># 隐藏状态h的特征数量</span></span></span><br><span class="line"><span class="params">        rnn_dropout_rate=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        num_layer=<span class="number">2</span>,  <span class="comment"># LSTM堆叠数量</span></span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available(<span class="params"></span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">        pad_size=PAD_SIZE</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(SiaGRU, self).__init__()</span><br><span class="line">        self.name = <span class="string">&#x27;SiaGRU&#x27;</span></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.word_emb = nn.Embedding(</span><br><span class="line">            num_embeddings=n_vocab,  <span class="comment"># 词表长度</span></span><br><span class="line">            embedding_dim=self.embed_dim,  <span class="comment"># 词嵌入维度</span></span><br><span class="line">            padding_idx=n_vocab-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.embed_dropout_rate = embed_dropout_rate</span><br><span class="line">        self.num_layer = num_layer</span><br><span class="line">        self.gru = nn.LSTM(</span><br><span class="line">            self.embed_dim,  <span class="comment"># 词嵌入维度</span></span><br><span class="line">            hidden_size,  <span class="comment"># 隐藏状态h的特征数量</span></span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span>,  <span class="comment"># 双向LSTM</span></span><br><span class="line">            num_layers=<span class="number">2</span>,  <span class="comment"># 堆叠2个BiLSTM</span></span><br><span class="line">            dropout=rnn_dropout_rate</span><br><span class="line">        )</span><br><span class="line">        self.h0 = self.init_hidden((<span class="number">2</span> * self.num_layer, <span class="number">1</span>, hidden_size)).to(device)</span><br><span class="line">        self.pred_fc = nn.Linear(pad_size, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_hidden</span>(<span class="params">size</span>):</span><br><span class="line">        h0 = nn.Parameter(torch.randn(size))</span><br><span class="line">        nn.init.xavier_normal_(h0)</span><br><span class="line">        <span class="keyword">return</span> h0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_once</span>(<span class="params">self, x</span>):</span><br><span class="line">        output, hidden = self.gru(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dropout</span>(<span class="params">self, v</span>):</span><br><span class="line">        <span class="keyword">return</span> F.dropout(v, p=self.embed_dropout_rate, training=self.training)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q1, q2</span>):</span><br><span class="line">        <span class="comment"># 词嵌入：batch_size * seq_len =&gt; batch_size * seq_len * dim</span></span><br><span class="line">        p_encode = self.dropout(self.word_emb(q1))</span><br><span class="line">        h_encode = self.dropout(self.word_emb(q2))</span><br><span class="line">        <span class="comment"># 词嵌入输入2个堆叠的BiLSTM</span></span><br><span class="line">        encoding1 = self.forward_once(p_encode)</span><br><span class="line">        encoding2 = self.forward_once(h_encode)</span><br><span class="line">        <span class="comment"># 计算相似度</span></span><br><span class="line">        sim = torch.exp(-torch.norm(encoding1 - encoding2, p=<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)).squeeze(dim=-<span class="number">1</span>)</span><br><span class="line">        x = self.pred_fc(sim)</span><br><span class="line">        probabilities = nn.functional.softmax(x, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x, probabilities</span><br></pre></td></tr></table></figure>
<h2 id="卷积和全连接缺点">卷积和全连接缺点</h2>
<ul>
<li>文本作为一个整体处理</li>
<li>输入输出固定</li>
</ul>
<h1 id="dropout">Dropout</h1>
<p>当训练一个深度神经网络时，我们可以随机丟弃一部分神经元(同时丢弃其对应的连接边)来避免过拟合，这种方法称为丢弃法Dropout Method)。每次选择丢弃的神经元是随机的。</p>
<p>每做一次丢弃，相当于从原始的网络中采样得到一个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数。那么，最终的网络可以近似看作集成了多个不同网络的组合模型。</p>
<h1 id="batch-normalization">Batch Normalization</h1>
<p>神经网络中应用BN时，是对神经网络输入数据的每一维分别进行归一化处理,使用多个输入样本的相同维数据计算出这一维数据的均值和方差,用于对本维数据进行归一化处理。</p>
<p>归一化可以有效提高训练效率的原因有以下几个方面:</p>
<p>更好的尺度不变性：在深度神经网络中，一个神经层的输入是之前神经层的输出。给定一个神经层l，它之前的神经层(1, ..., l-1)的参数变化会导致其输入的分布发生较大的改变。当使用随机梯度下降来训练网络时，每次参数更新都会导致该神经层的输入分布发生改变。越高的层，其输入分布会改变得越明显，就像一栋高楼， 低楼层发生一个较小的偏移，可能会导致高楼层较大的偏移。从机器学习角度来看，如果一个神经层的输入分布发生了改变，那么其参数需要重新学习，为了缓解这个问题，我们可以对每一个神经层的输入进行归一化操作，使其分布保持稳定。</p>
<p>更平滑的优化地形：逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面还可以使得神经网络的优化地形(Optimization Landscape)更加平滑，以及使梯度变得更加稳定，从而允许我们使用更大的学习率，并提高收敛速度。</p>
<h1 id="layer-normalization">Layer Normalization</h1>
<p>解决batch normalization的问题：</p>
<ul>
<li>对batch size非常敏感。batch normalization的一个重要出发点是保持每层输入的数据同分布。</li>
<li>不适用于序列数据。</li>
</ul>
<p>层归一化：针对单个样本对每一层内的神经元进行归一化</p>
<h1 id="循环神经网络">循环神经网络</h1>
<p>循环神经网络通过使用带自反馈的神经元，能够处理任意长度的时序数据。</p>
<p>长程依赖问题：由于梯度爆炸或消失问题，实际上只能学习到短周期的依赖关系。</p>
<h2 id="lstm">LSTM</h2>
<figure>
<img src="https://zh-v2.d2l.ai/_images/lstm-3.svg?ynotemdtimestamp=1655558969941" alt="LSTM" /><figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<p>在图中,Xt-1,Xt,Xt+1是三个神经元的输入,Xt表示的是当前细胞的输入。</p>
<p>ht表示的是当前细胞的输出。ht−1表示的是上一个cell的输出。ht加softmax即可作为真正输出，否则作为隐藏层。</p>
<p>σ表示Sigmoid激励函数。</p>
<p>tanh代表tanh激励函数。</p>
<p>×代表相乘操作,⊕代表相加操作,→代表向量传输。</p>
<p>LSTM关键：“细胞状态”：Ct</p>
<p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。LSTM使用“门”让信息选择性通过来控制“细胞状态”，来去除或者增加信息到细胞状态。包含一个sigmoid神经网络层和一个pointwise乘法操作。Sigmoid层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过”。</p>
<p>第1步：决定从“细胞状态”中丢弃什么信息：遗忘门</p>
<p>第2步：决定放什么新信息到“细胞状态”中：输入门、候选记忆</p>
<p>Sigmoid层决定什么值需要更新，Tanh层创建一个新的候选值向量，上述2步是为状态更新做准备。</p>
<p>第3步：更新“细胞状态”</p>
<p>更新Ct-1为Ct，把旧状态与遗忘门输出相乘，丢弃掉需要丢弃的信息，加上输入门输出与候选记忆的乘积得到新的候选值。</p>
<p>第4步：基于“细胞状态”得到输出</p>
<p>首先运行一个sigmoid层来确定细胞状态的哪个部分将输出，接着用tanh处理细胞状态(得到一个在-1到1之间的值)，再将它和sigmoid门的输出相乘，输出确定输出的部分。</p>
<h2 id="gru">GRU</h2>
<figure>
<img src="https://zh-v2.d2l.ai/_images/gru-3.svg" alt="GRU" /><figcaption aria-hidden="true">GRU</figcaption>
</figure>
<p>门限循环单元( Gated Recurrent Unit，GRU) 是一种比LSTM更加简化的版本。在LSTM中，输入门和遗忘门是互补关系，因为同时用两个门比较冗余。GRU将输入门与和遗忘门合并成一个门：更新门(Update Gate)，同时还合并了记忆单元和神经元活性。GRU 模型中有两个门：更新门z和重置门r。更新门z用来控制当前的状态需要遗忘多少历史信息和接受多少新信息。重置门r用来控制候选状态中有多少信息是从历史信息中得到。</p>
<h2 id="srnn">SRNN</h2>
<p>堆叠循环神经网络(Stacked Recurrent Neural Network，SRNN)</p>
<h2 id="bilstm">BiLSTM</h2>
<p>双向循环神经网络(Bidirectional Recurrent Neural Network，Bi-RNN)</p>
<h1 id="医学自然语言处理任务">医学自然语言处理任务</h1>
<ul>
<li>病历文本数据挖掘</li>
<li>医疗知识库构建</li>
<li>病历检索</li>
</ul>
<ol type="1">
<li>分词</li>
<li>词性标注</li>
<li>实体识别</li>
<li>实体标准化：不规范输入、不同数据来源或不同人员在表达相同或类似概念时描述方式多种多样。</li>
<li>实体关系抽取</li>
<li>语义分析：电子病历中出现大量长句</li>
</ol>
<h2 id="文本分类">文本分类</h2>
<h2 id="文本匹配">文本匹配</h2>
<p>例如信息检索、自动问答、机器翻译、对话系统、复述问题。</p>
<h2 id="词性标注">词性标注</h2>
<h2 id="命名实体识别-ner">命名实体识别 NER</h2>
<p>命名实体(named entity, NE) ，命名实体识别实际的任务主要是识别人名、地名、机构名，是实体关系抽取、信息检索、机器翻译、间答系统等多种自然语言处理技术必不可少的组成部分。医学数据源中提取出特定类型的命名实体，主要有疾病、药物.症状、检查、手术操作、器官部位等。</p>
<p>类别：</p>
<ul>
<li>B-Person（人名的开始部分)</li>
<li>I- Person(人名的中间部分)</li>
<li>B-Organization(组织机构的开始部分)</li>
<li>l-Organization(组织机构的中间部分)</li>
<li>O(非实体信息)</li>
</ul>
<p>发射分数（状态分数）：来自BiLSTM层的输出，为一个词作为不同类别的可能性。</p>
<p>转移分数矩阵：所有类别间转移的分数。</p>
<h3 id="crf层">CRF层</h3>
<p>CRF层可以加入一些约束来保证最终预测结果是有效的。这些约束可以在训练数据时被CRF层自动学习得到。有了这些有用的约束，错误的预测序列将会大大减少。</p>
<p>可能的约束条件有:</p>
<ul>
<li>句子的开头应该是“B-"或“O”，而不是“I-”</li>
<li>"B-label1 I-label2 I-label3..."在该模式中，类别1,2,3应该是同一种实体类别。比如"B-Person I-Person"是正确的，而“ B-Person l-Organization"则是错误的。</li>
<li>"O I-label"是错误的，命名实体的开头应该是“B-" 而不是“I-"。</li>
</ul>
<p>CRF损失函数只需要两个得分：真实路径得分和所有可能路径总得分，真实路径得分的比例应随着训练不断增加。</p>
<h3 id="维特比算法">维特比算法</h3>
<p>HMM（隐马尔可夫模型）用到了2个假设：</p>
<ul>
<li>齐次马尔可夫性假设。t时刻的隐状态只和t-1时刻的隐状态相关，与其他时刻状态无关。</li>
<li>观测独立性假设。任意时刻的观测值依赖于该时刻的马尔可夫链的隐状态，与其他观测及隐状态无关。</li>
</ul>
<p>HMM的3大参数：</p>
<ul>
<li>π：初始隐状态概率向量</li>
<li>A：状态转移概率矩阵</li>
<li>B：发射概率（观测）矩阵</li>
</ul>
<p>维特比算法：在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率是从前一时刻哪一个隐状态转移过来的，最后再从结尾回溯最大概率，也就是最有可能的最优路径。</p>
<h2 id="关系抽取">关系抽取</h2>
<h2 id="情感分析">情感分析</h2>
<p>(Sentiment analysis)又称倾向性分析、意见抽取( Opinion extraction) 、意见挖掘( Opinion mining)、情感挖掘( Sentiment mining)、主观分析( Subjectivity analysis)等，它是对带有 情感色彩的主观性文本进行分析、处理、归纳和推理的过程。</p>
<ul>
<li>褒贬(中)分类：
<ul>
<li>一句话是褒义还是贬义</li>
</ul></li>
<li>细粒度分类：
<ul>
<li>喜怒悲恐惊(微博情绪指数系统)</li>
<li>将情感极性打分(例如1-5颗星)</li>
<li>褒贬中更细化(强褒义、褒义.些许褒义等)</li>
</ul></li>
<li>进阶分类：
<ul>
<li>一评价词 (Opinion)、 评价对象(Target)抽取</li>
<li>复杂观点抽取</li>
</ul></li>
</ul>
<h2 id="文本生成">文本生成</h2>
<p>如何构造样本训练RNN？切分语句，每一个切分作为输入，切分的下一个词作为输出。</p>
<p>主要挑战：</p>
<ul>
<li><p>一致性:内容应当准确忠实，不应语句前后矛盾、或与输入相悖</p></li>
<li><p>流畅性:语言应通顺易懂，不应不成句子</p></li>
<li><p>多样性:尽可能符合人类表达习惯，表达不应过于单一，尽可能提现情感倾向</p></li>
</ul>
<h2 id="文本摘要">文本摘要</h2>
<h2 id="问答系统">问答系统</h2>
<h2 id="看图说话">看图说话</h2>
<h1 id="seq2seq">Seq2Seq</h1>
<p>机器翻译(machine translation, MT)是用计算机把一种语言 (源语言, source language)翻译成另一种语言(目标语言, target language)的一门学科和技术。</p>
<p>机器翻译的困难</p>
<ul>
<li>自然语言中普遍存在的歧义和未知现象
<ul>
<li>句法结构歧义/词汇歧义/语用歧义..</li>
<li>新的词汇、术语、结构、语义..</li>
</ul></li>
<li>机器翻译不仅仅是字符串的转换
<ul>
<li>不同语言之间文化的差异</li>
<li>现有方法无法表示和利用世界知识和常识</li>
</ul></li>
<li>机器翻译的解不唯一，而且始终存在的人为的标准</li>
</ul>
<h2 id="编码器-解码器">编码器-解码器</h2>
<p>编码器读入源语言句子，生成源语言句子的向量表示；解码器基于源语言句子的向量表示生成目标语言句子。一体化模型(不存在独立的语言模型和翻译模型)</p>
<p>编码器的作用是把一个不定长的输入文本变换成一个定长的向量。</p>
<p>解码器的作用是把定长向量进行传递并输出不定长的输出文本。</p>
<p>形式化地，给定由t个单词组成的输入序列文本X = (x1, x2.... xt)，以及由t个单词组成的输出序列文本Y = (y1, y2.... yt)，“编码器 -解码器”框架就是使用最大似然估计方法，建模p(Y|X)。使用编码器中的循环神经网络计算得到输入序列的隐藏状态集合{h1, h2....ht}，解码器读取输入文本的状态，按序输出每个目标单词yj。</p>
<h3 id="搜索方式">搜索方式</h3>
<p>贪心搜索：在贪心策略中，遇到结束符就停止生成，例如:<START> he hit me with a pie <END></p>
<p>穷举搜索</p>
<p>束搜索：在束搜索策略中，会产生很多不同的语句。当一条语句遇到结束符，这个假设就停止。存储当前停止的假设，继续寻找其他语句。</p>
<h3 id="采样方式">采样方式</h3>
<p>随机采样：温度小的时候，倾向于使用常规词汇，而温度大的时候倾向于使用与上下文相关的词汇。</p>
<p>Top-K采样：挑选概率最高k个token， 然后重新过softmax 算概率，之后根据获得概率进行采样，接着进行下一步生成，不断重复。</p>
<p>Top-P采样：先设置一个概率界限，如p=0.9 ，然后从最大概率的token往下开始取，同时将概率累加起来，当取到大于等于p也就是0.9 时停止。</p>
<h2 id="训练方式">训练方式</h2>
<h3 id="free-running">Free running</h3>
<p>将解码器的猜测作为每个下一个输入。（预测时只能Free running）</p>
<h3 id="teacher-forcing">Teacher forcing</h3>
<p>将实际目标输出作为每个下一个输入，而不是将解码器的猜测用作下一个输入。使用Teacher forcing会使其更快地收敛。</p>
<h3 id="scheduled-sampling">scheduled sampling</h3>
<p>每一步以概率p靠自己上一步的输入来预测，以概率1-p根据老师的提示来预测</p>
<h2 id="attention">Attention</h2>
<figure>
<img src="../images/attention.png" alt="attention" /><figcaption aria-hidden="true">attention</figcaption>
</figure>
<p>==<span class="math inline">\(Attention(Q, K, V)=SoftMax(\frac{QK^T}{\sqrt{d_k}})V\)</span>==，<span class="math inline">\(d_k\)</span>表示K的维度</p>
<p><span class="math inline">\(d_k\)</span>较大时，向量内积容易取很大的值（query和key向量中的元素都是相互独立的均值为0，方差为1的随机变量，那么这两个向量的内积的均值为0，而方差为<span class="math inline">\(d_k\)</span>），SoftMax函数梯度很小趋于0。</p>
<p>除以<span class="math inline">\(\sqrt{d_k}\)</span>使得方差稳定到1，SoftMax的梯度不至于太小，有效控制了梯度消失问题。</p>
<h1 id="transformer">Transformer</h1>
<figure>
<img src="https://zh-v2.d2l.ai/_images/transformer.svg" alt="transformer" /><figcaption aria-hidden="true">transformer</figcaption>
</figure>
<p>Transformer包含一个编码器encoder和一个解码器decoder。</p>
<ul>
<li>编码器encoder包含一组6个相同的层Layer，每层包含两个子层Sublayer：
<ul>
<li>第一个子层是一个多头自注意力multi-head self attention层。</li>
<li>第二个子层是一个简单的全连接层。</li>
<li>每个子层都使用残差直连，并且残差直连之后跟随一个layer normalization。</li>
</ul></li>
<li>解码器decoder也包含一组6个相同的层Layer，但是每层包含三个子层Sublayer：
<ul>
<li>第一个子层也是一个多头自注意力multi-head masked self attention 层。</li>
<li>第二个子层是一个多头注意力multi-head attention层，用于捕获decoder output和encoder output之间的attention。</li>
<li>第三个子层是一个简单的全连接层。</li>
<li>每个子层都使用残差直连，并且残差直连之后跟随一个layer normalization。</li>
</ul></li>
</ul>
<h2 id="位置编码">位置编码</h2>
<p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，可以通过在输入表示中添加位置编码（positional encoding）来注入绝对的或相对的位置信息。位置编码不是模型架构的一部分，而只是预处理的一部分。生成的位置编码向量与每个词的嵌入向量大小维度相同。</p>
<p>常使用<strong>基于正弦函数和余弦函数的固定位置编码</strong>。</p>
<p>性质：</p>
<ul>
<li>两个位置编码的点积(dot product)仅取决于偏移量，也即两个位置编码的点积可以反应出两个位置编码间的距离。</li>
<li>位置编码的点积是无向的</li>
</ul>
<h2 id="self-attention">Self-Attention</h2>
<figure>
<img src="../images/self-attention.png" alt="self-attention" /><figcaption aria-hidden="true">self-attention</figcaption>
</figure>
<p>若更换输入X的顺序：Query、Key向量、Value向量和输出向量没有发生变化，只是顺序发生变化</p>
<h2 id="multi-head-attention">Multi-head Attention</h2>
<p>Multi-head Attention的本质是，在参数总量保持不变的情况下，将同样的query, key, value映射到原来的高维空间的不同子空间中进行attention的计算，在最后一步再合并不同子空间中的attention信息。</p>
<p>这样降低了计算每个head的attention时每个向量的维度，在某种意义上防止了过拟合；由于Attention在不同子空间中有不同的分布，Multi-head Attention实际上是寻找了序列之间不同角度的关联关系，并在最后concatenation这一步骤中，将不同子空间中捕获到的关联关系再综合起来。</p>
<p>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。</p>
<h2 id="masked-attention">Masked Attention</h2>
<p>为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_ step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。</p>
<h1 id="自回归与非自回归建模">自回归与非自回归建模</h1>
<figure>
<img src="../images/自回归与非自回归.png" alt="自回归与非自回归" /><figcaption aria-hidden="true">自回归与非自回归</figcaption>
</figure>
<p>自回归：</p>
<p>优点：</p>
<ul>
<li>使用条件概率链式分解输出序列的联合分布，分解后的每项条件概率的分布更加简单，更容易被模型建模和拟合。</li>
<li>自回归建模方式模仿了序列产生的本质。</li>
<li>不需要显式建模源序列和目标序列之间的长度关系。</li>
<li>生成的效果较好（目前大多数序列生成任务的最好模型都是自回归的）。</li>
</ul>
<p>缺点：</p>
<ul>
<li>在生成序列时需要逐步生成，使得生成速度较慢，无法高效利用硬件的并行性。</li>
<li>在训练和生成时存在一定的差距（训练时用真实的上一时间步输出作为下一个时间步输入，而生成时使用模型上一时间步生成的输出作为下一个时间步输入）</li>
</ul>
<p>非自回归：</p>
<p>优点：</p>
<ul>
<li>一次性并行生成序列，而不需要逐步生成，充分利用了硬件的并行性，具有较快的生成速度。</li>
<li>直接建模联合分布，在训练和生成时均使用的输入数据一致,因此训练和生成差距较小。</li>
<li>建模方式简单，模型逻辑简单。</li>
</ul>
<p>缺点：</p>
<ul>
<li>联合分布通常具有多峰性(multimodal)，即对于一个X，可能存在多个合理的Y，大多数的损失函数难以刻画一个联合分布的多峰性，导致生成序列会存在过渡平滑(over-smoothing)问题。</li>
<li>跨模态翻译时，存在源序列和目标序列长度不一致问题。</li>
<li>在大多数任务中，性能上仍较难超过最好的自回归模型。</li>
</ul>
<h1 id="机器翻译评价标准">机器翻译评价标准</h1>
<h2 id="困惑度">困惑度</h2>
<p>困感度(Perplexity)是信息论的一个概念，可以用来衡量一个分布的不确定性。给定一个测试文本集合，一个好的序列生成模型应该使得测试集合中的句子的联合概率尽可能高。</p>
<p>困惑度可以衡量模型分布与样本经验分布之间的契合程度。<strong>困惑度越低则两个分布越接近</strong>。</p>
<p>困感度是与语料有关，两种语 言模型只有在使用相同评价语料的情况下才可以比较困惑度。</p>
<h2 id="belu">BELU</h2>
<p>BLEU (Bilingual Evaluation Understudy)是衡量模型生成序列和参考序列之间的N元词组(N-Gram)的重合度，最早用来评价机器翻译模型的质量，目前也广泛应用在各种序列生成任务中。</p>
<p>重合度越高越好，BELU分值0~1，越高译文质量越好。</p>
<h1 id="迁移学习">迁移学习</h1>
<p>把在某个领域获取的知识迁移到对另一个领域的学习中，也就是举一反三</p>
<ul>
<li>大数据与少标注之间的矛盾：迁移数据标注</li>
<li>大数据与弱计算之间的矛盾：模型迁移</li>
<li>普适化模型与个性化需求之间的矛盾：自适应学习</li>
<li>特定应用的需求：相似领域知识迁移</li>
</ul>
<p>迁移学习目的是借助一个源数据集来学习目标数据集的一个预测函数。</p>
<p>我们按照学习方法把迁移学习分成样本迁移、特征迁移、模型迁移和关系迁移四个类别：</p>
<ul>
<li>样本迁移：“各取所需”：剔除可能产生误导的样本；而对于特征相似、对任务有帮助的样本，则让其扩充训练数据，充分做到物尽其用。</li>
<li>特征迁移：通过引入源数据特征来帮助完成目标数据特征域的机器学习任务。
<ul>
<li>特征迁移通常假设源域和目标域间有一些交叉特征，是从共同的特征空间的角度迁移知识。</li>
</ul></li>
<li>模型迁移：从源域和目标域中找到他们之间共享的参数信息，以实现迁移的方法。
<ul>
<li>这种迁移方式要求的假设条件是源域中的数据与目标域中的数据可以共享一些模型的参数。</li>
</ul></li>
<li>关系迁移：源域和目标数据域之间关系的相似性，通过从源领域挖掘与目标数据相关的关系模式，帮助在目标数据上进行机器学习任务。</li>
</ul>
<h1 id="多任务学习">多任务学习</h1>
<p>多任务学习(Multi-task Learning)是指同时学习多个相关任务，让这些任务在学习过程 中共享知识，利用多个任务之间的相关性来改进模型在每个任务上的性能和泛化能力。</p>
<ul>
<li>硬共享模式：让不同任务的神经网络模型共同使用一些共享模块(一般是低层)来提取一些通用特征， 然后再针对每个不同的任务设置一些私有模块(一般是高层)来提取一些任务特定的特征。</li>
<li>软共享模式：不显式地设置共享模块，但每个任务都可以从其他任务中“窃取”一些信息来提高 自己的能力，窃取的方式包括直接复制使用其他任务的隐状态，或使用注意力机制来主动选取有用的信息。</li>
<li>层次共享模式：一般神经网络中不同层抽取的特征类型不同，低层一般抽取一些低级的局部特征，高层抽取一些高级的抽象语义特征，因此如果多任务学习中不同任务也有级别高低之分，那么一个合理的共享模式是让低级任务在低层输出，高级任务在高层输出。</li>
<li>共享-私有模式：一个更加分工明确的方式是将共享模块和任务特定(私有)模块的责任分开， 共享模块捕捉一些跨任务的共享特征，而私有模块只捕捉和特定任务相关的特征，最终的表示由共享特征和私有特征共同构成。</li>
</ul>
<p>多任务学习的流程可以分为两个阶段:</p>
<ul>
<li>联合训练阶段：每次迭代时，随机挑选一个任务，然后从这个任务中随机选择一些训练样本，计算梯度并更新参数。</li>
<li>单任务微调阶段：基于多任务学习得到的参数，分别在每个单独任务进行微调(Fine-Tuning)。其中单任务精调阶段为可选阶段。当多个任务的差异性比较大时，在每个单任务上继续优化参数可以进一步提升模型能力。</li>
</ul>
<p>多任务学习通常可以获得比单任务学习更好的泛化能力，主要有以下几个原因:</p>
<ul>
<li>多任务学习在多个任务的数据集上进行训练，比单任务学习的训练集更大，由于多个任务之间有一定的相关性，因此多 任务学习相当于是一种隐式的数据增强，可以提高模型的泛化能力。</li>
<li>多任务学习中的共享模块需要兼顾所有任务，这在一定程度上避免了模型过拟合到单个任务的训练集，可以看作一 种正则化。</li>
<li>既然一个好的表示通常需要适用于多个不同任务，多任务学习的机制使得它会比单任务学习获得更好的表示。</li>
<li>在多任务学习中， 每个任务都可以“选择性”利用其 他任务中学习到的隐藏特征，从而提高自身的能力。</li>
</ul>
<h1 id="预训练模型">预训练模型</h1>
<p>静态词向量的缺点：</p>
<ul>
<li>难以表达一词多义。</li>
<li>难以表达不同的语法或语义信息。</li>
<li>词向量是静态的，无法根据上下文进行调整。</li>
<li>词向量表示只有一个向量，糅合了太多信息，粒度较粗。</li>
</ul>
<p>词向量模型需要解决两个问题:</p>
<ul>
<li>词使用的复杂特性，如句法和语法。</li>
<li>如何在具体的语境下使用词，比如多义词的问题。</li>
</ul>
<h2 id="elmo">ELMo</h2>
<p>Embeddings from Language Models 基于语言模型的词嵌入</p>
<p>ELMo世界里，预训练好的模型不再只是向量对应关系，而是一个训练好的模型。使用时，将一句话或一段话输入模型， 模型会根据上下文来推断每个词对应的词向量。这样做之后明显的好处之一就是对于多义词，可以结合前后语境对多义词进行理解。</p>
<p>ELMO的结构：由一层input层和两层LSTM组合而成的</p>
<p>ELMO预训练：ELMO的三层每一层都有一个输出，将这三层的输出按比例相加后即为所得vector。 这个比例是模型学习得到的。得到加权后的向量后，如何使用取决于任务的效果。</p>
<h2 id="bert">BERT</h2>
<p>Bidirectional Encoder Representations from Transformers 基于Transformer的双向编码器表示</p>
<p>BERT结构：只有编码器的transformer</p>
<p>BERT输入：</p>
<ul>
<li>在BERT中，永远都将第一个位置输入分类提示符[CLS],如果执行的是分类任务,第一个位置最终会输出一个向量，作为分类依据。对于任何任务, BERT的输入永远都是将-对句子放在同一个序列中，无论这对句子是真正连续的上下文还是随机拼接的. BERT句子和句子之间用分隔符[SEP]隔开，在结尾也要加上一个[SEP]。</li>
<li>加入额外的片段嵌入</li>
<li>位置编码可学习</li>
</ul>
<p>BERT预训练：</p>
<ul>
<li>带掩码的语言模型：Transformer的编码器是双向，标准语言模型要求单向。带掩码的语言模型每次随机(15%概率)将一些词元换成<mask>因为微调任务中不出现<mask>：80%概率下，将选中的词元变成<mask>、10%概率下换成一个随机词元、10%概率下保持原有的词元</li>
<li>下一句子预测：预测一个句子对中两个句子是不是相邻。训练样本中：50%概率选择相邻句子对、50%概率选择随机句子对，将<cls>对应的输出放到一个全连接层来预测。</li>
</ul>
<p>BERT微调：</p>
<ul>
<li>句子分类：将<cls>对应的向量输入到全连接层分类。</li>
<li>命名实体识别：将非特殊词元放进全连接层分类</li>
<li>文本区间预测（给定问题和一段话，预测答案在这段话中的位置）：对片段中的每个词元预测它是不是回答的开头或结束</li>
</ul>
<h2 id="gpt">GPT</h2>
<p>Generative Pre-Training</p>
<p>GPT结构：只有解码器的transformer</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-07-01</span><i class="fa fa-tag"></i><a class="tag" href="/categories/笔记/" title="笔记">笔记 </a><a class="tag" href="/tags/NLP/" title="NLP">NLP </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://KinNiji.github.io/自然语言处理/,NijiBlog,自然语言处理,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9/" title="自然语言处理重点">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>